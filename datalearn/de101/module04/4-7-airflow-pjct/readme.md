# Реализация ETL процесса на Apache AirFlow

## Задачи проекта

Получить информацию о рынке интересующих вакансий и динамику этого рынка с течением времени

## Этапы разработки

1. ['Разработать схему архетектуры решения'](#разработка-архетектуры-решения)
2. ['Создать схему хранилтща данных'](#создание-схемы-хранилища-данных)
3. ['Определить необходимые этапы ETL-процесса'](#определение-этапов-etl-процесса)
4. ['Разработка ETL процесса'](#разработка-etl-процесса)
5. ['Провести тестирование разработанного конвейера данных'](#тестирование-решения-на-локальной-машине)
6. ['Автономность решения'](#автономность-решения)

## Реализация

Определив цели и этапы приступаем к реализации

### Разработка архетектуры решения

Информацию по интересуюшим нас вакансиям будем получать используя открытый API hh.ru. Используя данный API можно получать данные об открытых вакансиях по нужным критериям формируя запросы. В ответ на запрос сервис предоставляет данные в формате JSON по всем открытим на момент запроса вакансиям. Отслеживая изменения можно составить представление о динамике рынка.

Разработаем схему решения.

![схема](./img/DataFlow.JPG)

На схеме представлен "поток" данных, который полностью управляется Apache Airflow. Сначала данные забираются с помощью API и транформируются на машине с аркестратором и времменым хранилищем. После этого после этого загружаются в STG для дальнейшего заполнения DW. Затем обновляются витрины данных Data Mart и обновляется информация в BI

### Создание схемы хранилища данных

#### Изучение получаемых данных

Для начала определим какие данные мы получаем.

Выполним запрос: `https://api.hh.ru/vacancies?text=data%20engineer&search_field=name` и сохраним полученный [файл](./src/vacancies.json) для анализа

Рассмотрим полученную информацию в сжатом виде

```JSON
"items":{...},
"found":467,
"pages":24,
"per_page":20,
"page":0,
"clusters":null,
"arguments":null,
"alternate_url":"https://hh.ru/search/vacancy?enable_snippets=true&search_field=name&text=data+engineer"}
```

информацию по всем частям можно найти в документации, поэтому, пробежимся по тому что нам интересно:

`items` - словари с информацией о вакансиях
`found` - найденые вакансии
`pages` - доступные страницы

перейдем в items и возьмем что нам нужно из описания вакансий

`id` - id вакансии
`name` - название
`area` - местоположение
`salary` - данные о зарплате
`created_at` - дата создания
`posted_at` - дата размещения
`alternate_url` - ссылка на вакансию на hh.ru
`employer` - информация о работодателе
`schedule` - тип занятости

#### Определяем структуру

Мы будем складывать данные в таблицу staging для хранения всех полученных данных.

Далее мы переносим их в dw где формируем dimensional и fact таблицы.

Учитывая специфику там потребуется сравнивать получаемые и имеющиеся данные между собой чтобы отслеживать изменения в открытых вакансиях и отмечать дату изменений.

!['Схема_DW'](./img/dw_schema.JPG)

### Определение этапов ETL-процесса

Так как для реализации данного проекта мы выбрали AirFlow, разобьем сразу весь процесс на DAG'и и tasks

#### DAG_01 Получение данных

Для начала нужно получить данные

##### task_01 Генерируем запросы и сохраняем файлы ответов

Получаем список интересующих вакансий из списка
    Выполняем запрос на каждую вакансию
        Получаем количество страниц
            Проходим по сем страницам и сохраняем файлы с ответами

##### task_02 Объединение данных

Проходим по файлам
    Выбираем из файла необходимые данные
    Добавляем в общий DataFrame

Добавляем дату
Определяем тип данных в столбцах
Сохраняем DataFrame в csv

#### DAG_02 Загрузка данных в STG

У нас теперь имеется общая таблица с данными и мы будем загружать её в STG

##### tack_01 чтение файла и загрузка

Открываем файл
Подключаемся к базе данных
Добавляем данные в таблицу STG

#### DAG_03 Update DW

##### task_01 Сравнение и update dim таблиц

##### task_02 Сравнение и update fact таблиц

#### DAG_04 Update Data marts

##### task_01 Update

### Разработка ETL процесса

### Тестирование решения на локальной машине

### Автономность решения
